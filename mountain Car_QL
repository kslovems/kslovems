import gym
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import random

env = gym.make('MountainCar-v0')

n_action = env.action_space.n
env_low = env.observation_space.low
env_high = env.observation_space.high
epsilon = 0.2
slots = 30 # 이산적인 정보 갯수 설정 
num_episodes = 10000 # 에피소드 숫자 설정 
alpha = 0.1 # learning rate 설정 
gamma = 0.99 # discount factor 설정   
    
# 연속적인 위치와 속도정보를 이산적인 정보로 수정함 
def getState(state, env_low = env_low, env_high = env_high, slots = slots):
    discretized_env = (env_high - env_low) / slots
    discretized_position = int((state[0] - env_low[0]) / discretized_env[0])
    discretized_velocity = int((state[1] - env_low[1]) / discretized_env[1])
    return discretized_position, discretized_velocity

# action_epsilon_greedy 함수 
def action_epsilon_greedy(pos, vel, q_table, epsilon = epsilon):
    if np.random.rand() < epsilon: #explore 실행  
        return env.action_space.sample()
    else: 
        return np.argmax(q_table[pos][vel])

def main(): 
    
    global epsilon 
    epsilon = epsilon

     # q-table 설정 및 초기화
    q_table_q = np.zeros((slots, slots, env.action_space.n))
    rewards_q = []
    plot_reward = np.zeros(num_episodes)  # 정보를 위한 행렬
    
    # training 과정 
    for ep in range(num_episodes):
    
        state = env.reset()
        current_reward = 0
        done = False
        # 위치 정보와 속도정보 이산화 정보 변환
        pos, vel = getState(state)
        while not done:

            action = action_epsilon_greedy(pos, vel, q_table_q)
            next_state, reward, done, info = env.step(action)
            next_pos, next_vel = getState(next_state)

            if done and next_state[0] >= env.goal_position:
                q_table_q[next_pos][next_vel][action] = reward
            else:
                # Q-learning 공식에 따라  Q value 업데이트: Q(S, A) <-- Q(S, A) + alpha [R + gamma * Q(S', A') - Q(S, A)]
                q_table_q[pos][vel][action] += \
                alpha * (reward + gamma * np.max(q_table_q[next_pos][next_vel]) - q_table_q[pos][vel][action])
            # 재할당 state, action, reward
            state = next_state
            pos, vel = next_pos, next_vel
            current_reward += reward

            # epsilon 업데이트
            if epsilon > 0:
              epsilon*= (num_episodes - 2)/num_episodes
    
            rewards_q.append(current_reward)
            plot_reward[ep] = current_reward

    # 결과 프린트 
    plt.plot(np.arange(num_episodes),plot_reward)
    plt.show()    
    env.close()

if __name__ == "__main__":
    main()
