import gym
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import random

env = gym.make('MountainCar-v0')

n_action = env.action_space.n
env_low = env.observation_space.low
env_high = env.observation_space.high
epsilon = 0.5
slots = 30 # 이산적인 정보 갯수 설정 
num_episodes = 10000 # 에피소드 숫자 설정 
alpha = 0.1 # learning rate 설정 
gamma = 0.99 # discount factor 설정    

# 연속적인 위치와 속도정보를 이산적인 정보로 수정함 
def getState(state, env_low = env_low, env_high = env_high, slots = slots):
    discretized_env = (env_high - env_low) / slots
    discretized_position = int((state[0] - env_low[0]) / discretized_env[0])
    discretized_velocity = int((state[1] - env_low[1]) / discretized_env[1])
    return discretized_position, discretized_velocity

# choose action
def action_epsilon_greedy(pos, vel, q_table, epsilon = 0.5):
    if np.random.rand() < epsilon: # explore
        return env.action_space.sample()
    else: 
        return np.argmax(q_table[pos][vel])
def main(): 
    
    global epsilon 
    epsilon = epsilon

     # q-table 설정 및 초기화 
    q_table_sarsa = np.zeros((slots, slots, env.action_space.n))
    rewards_sarsa = []
    plot_reward = np.zeros(num_episodes) # 정보를 위한 행렬 
    

    # training 과정  
    for ep in range(num_episodes):
        current_reward = 0
        done = False
        state = env.reset()
        # 위치 정보와 속도정보 이산화 정보 변환 
        pos, vel = getState(state)
        # epsilon_greedy 
        action = action_epsilon_greedy(pos, vel, q_table_sarsa)
        while not done:

            next_state, reward, done, info = env.step(action)
            next_pos, next_vel = getState(next_state)           
            next_action = action_epsilon_greedy(next_pos, next_vel, q_table_sarsa)
            if done and next_state[0] >= env.goal_position:
                q_table_sarsa[next_pos][next_vel][action] = reward
            else:
                #  SARSA 공식에 따라  Q value 업데이트: Q(S, A) <-- Q(S, A) + alpha [R + gamma * Q(S', A') - Q(S, A)]
                q_table_sarsa[pos][vel][action] += \
                alpha * (reward + gamma * q_table_sarsa[next_pos][next_vel][next_action] - q_table_sarsa[pos][vel][action])
        
            # state, action, reward 재할당 
            state = next_state
            pos, vel = next_pos, next_vel
            action = next_action
            current_reward += reward
    
          # epsilon 업데이트 
            if epsilon > 0:
                epsilon*= (num_episodes - 1)/num_episodes

            rewards_sarsa.append(current_reward)
            plot_reward[ep] = current_reward
            
    # 결과 프린트 
    plt.plot(np.arange(num_episodes),plot_reward)
    plt.show()
    
    env.close()
    
if __name__ == "__main__":
    main()
